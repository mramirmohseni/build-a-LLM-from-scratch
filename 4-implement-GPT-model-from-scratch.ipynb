{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPBc3HanB1jzzPyss1Si/Up"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 4: Implementing a GPT model from Scratch To Generate Text"],"metadata":{"id":"maXEtF3pYu1W"}},{"cell_type":"code","source":["from importlib.metadata import version\n","\n","print(\"matplotlib version:\", version(\"matplotlib\"))\n","print(\"torch version:\", version(\"torch\"))\n","print(\"tiktoken version:\", version(\"tiktoken\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EKhizzBaZumk","executionInfo":{"status":"ok","timestamp":1752403756115,"user_tz":-210,"elapsed":11,"user":{"displayName":"Amir Mohseni","userId":"02502065642708542298"}},"outputId":"dff6de2c-3f98-4225-c224-19ebe9b8e67d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["matplotlib version: 3.10.0\n","torch version: 2.6.0+cu124\n","tiktoken version: 0.9.0\n"]}]},{"cell_type":"markdown","source":["- In this chapter, we implement a GPT-like LLM architecture; the next chapter will focus on training this LLM\n","\n","<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/01.webp\" width=\"500px\">"],"metadata":{"id":"k9UfFb16Z0A3"}},{"cell_type":"markdown","source":["## 4.1 Coding an LLM architecture"],"metadata":{"id":"BAvhCY74aCV3"}},{"cell_type":"markdown","source":["- Chapter 1 discussed models like GPT and Llama, which generate words sequentially and are based on the decoder part of the original transformer architecture\n","- Therefore, these LLMs are often referred to as \"decoder-like\" LLMs\n","- Compared to conventional deep learning models, LLMs are larger, mainly due to their vast number of parameters, not the amount of code\n","- We'll see that many elements are repeated in an LLM's architecture\n","\n","<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/02.webp\" width=\"400px\">"],"metadata":{"id":"0SyGwG3AaD1c"}},{"cell_type":"markdown","source":["- In previous chapters, we used small embedding dimensions for token inputs and outputs for ease of illustration, ensuring they fit on a single page\n","- In this chapter, we consider embedding and model sizes akin to a small GPT-2 model\n","- smallest GPT-2 model: 124 million parameter\n","it also has 345, 762 million and also 1542 million parameter\n","\n","- we are focusing on GPT-2 because OpenAI has made the weights of the pretrained model publicly available\n","\n","- GPT-3 is fundamentally the same in terms of model architecture, except that it is scaled up from 1.5 billion parameters in GPT-2 to 175 billion parameters in GPT-3, and it is trained on more data. As of this writing, the weights for GPT-3 are not publicly available\n","\n","- GPT-2 is also a better choice for learning how to implement LLMs, as it can be run on a single laptop computer, whereas GPT-3 requires a GPU cluster for training and inference. According to Lambda Labs, it would take 355 years to train GPT-3 on a single V100 datacenter GPU, and 665 years on a consumer RTX 8000 GPU."],"metadata":{"id":"UhpWUt4RaQo0"}},{"cell_type":"markdown","source":["- Configuration details for the 124 million parameter GPT-2 model include:"],"metadata":{"id":"T1k1GRwcaUrn"}},{"cell_type":"code","source":["GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 1024, # Context length\n","    \"emb_dim\": 768,         # Embedding dimension\n","    \"n_heads\": 12,          # Number of attention heads\n","    \"n_layers\": 12,         # Number of layers\n","    \"drop_rate\": 0.1,       # Dropout rate\n","    \"qkv_bias\": False       # Query-Key-Value bias\n","}"],"metadata":{"id":"kxtDCwXpaVDB","executionInfo":{"status":"ok","timestamp":1752404090845,"user_tz":-210,"elapsed":5,"user":{"displayName":"Amir Mohseni","userId":"02502065642708542298"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["- We use short variable names to avoid long lines of code later\n","- `\"vocab_size\"` indicates a vocabulary size of 50,257 words, supported by the BPE tokenizer discussed in Chapter 2\n","- `\"context_length\"` represents the model's maximum input token count, as enabled by positional embeddings covered in Chapter 2\n","- `\"emb_dim\"` is the embedding size for token inputs, converting each input token into a 768-dimensional vector\n","- `\"n_heads\"` is the number of attention heads in the multi-head attention mechanism implemented in Chapter 3\n","- `\"n_layers\"` is the number of transformer blocks within the model, which we'll implement in upcoming sections\n","- `\"drop_rate\"` is the dropout mechanism's intensity, discussed in Chapter 3; 0.1 means dropping 10% of hidden units during training to mitigate overfitting\n","- `\"qkv_bias\"` decides if the `Linear` layers in the multi-head attention mechanism (from Chapter 3) should include a bias vector when computing query (Q), key (K), and value (V) tensors; we'll disable this option, which is standard practice in modern LLMs; however, we'll revisit this later when loading pretrained GPT-2 weights from OpenAI into our reimplementation in chapter 5"],"metadata":{"id":"KSkUuxx8bDhc"}},{"cell_type":"markdown","source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/03.webp\" width=\"500px\">"],"metadata":{"id":"U6QK489gmaYw"}},{"cell_type":"markdown","source":["- we will start by implementing a GPT placeholder architecture (DummyGPTModel) in this section. This will provide us with a big-picture view of how everything fits together and what other components we need to code in the upcoming sections to assemble the full GPT model architecture."],"metadata":{"id":"BGijdHjwma5a"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","\n","class DummyGPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        # Use a placeholder for TransformerBlock\n","        self.trf_blocks = nn.Sequential(\n","            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","\n","        # Use a placeholder for LayerNorm\n","        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n","        self.out_head = nn.Linear(\n","            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n","        )\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits\n","\n","\n","class DummyTransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        # A simple placeholder\n","\n","    def forward(self, x):\n","        # This block does nothing and just returns its input.\n","        return x\n","\n","\n","class DummyLayerNorm(nn.Module):\n","    def __init__(self, normalized_shape, eps=1e-5):\n","        super().__init__()\n","        # The parameters here are just to mimic the LayerNorm interface.\n","\n","    def forward(self, x):\n","        # This layer does nothing and just returns its input.\n","        return x"],"metadata":{"id":"yG5pyUXCnH42","executionInfo":{"status":"ok","timestamp":1752407352250,"user_tz":-210,"elapsed":7720,"user":{"displayName":"Amir Mohseni","userId":"02502065642708542298"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/04.webp?123\" width=\"500px\">"],"metadata":{"id":"qWfmn1KLnr8p"}},{"cell_type":"code","source":[],"metadata":{"id":"nvXJA2ZTngM7"},"execution_count":null,"outputs":[]}]}